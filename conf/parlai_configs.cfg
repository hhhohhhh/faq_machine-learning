
[main]
task_name = polyencoder
task = sentence_similarity
# biencoder
parlai_mode = interactive_web
mode = eval
zoo_model = zoo:pretrained_transformers/bi_model_huge_wikito/model
# zoo:tutorial_transformer_generator/model
model_name = bi_model_huge_wikito

# bert_ranker/poly_encoder_ranker
# biencoder # ployencoder

# bert_ranker/bi_encoder_ranker
# bert_ranker/bi_encoder_ranker 这个模型是使用 transformer 库构建的 biencoder
# transformer/biencoder 这个模型是 parlai 自己定义的模型，不需要 transformer 库
gpu_no = 0

[train]
# 自定义 task sentence_similarity
corpus= corpus
dataset_name =  dstc7
datapath = data/sentence_pairdata_to_dstc7_003
output_dir = output
# 增加 biendocer 对于bert的设定
transformer_model = bert-base-chinese
pretrained_dir = pretrained_model
init_model = pretrained_model/bert-base-chinese/pytorch_model.bin
# 提供 candidates 的方式： train 的时候默认为 'batch',eval 的模式默认为 inline
candidates= batch
eval_candidates = inline
fixed_candidates_filename = test_fixed_candidates_without_next_candidate.txt

# bert_ranker_biencoder 输出的维度
out_dim = 768

num_epochs = 8
batchsize = 32
eval_batchsize = 16
max_train_time = 200000

optimizer = adamw
lr = 5e-05
clip= -1
lr_scheduler_patience = 0
lr_scheduler_decay = 0.4
warmup_updates = 100

fp16 = True
data_parallel = False

history_size = 20
# 之前是 72
label_truncate = 128
# 之前是 360
text_truncate = 128


# 参数在 TrainModel 类中设置
validation-every-n-epochs = 0.5
validation-max-exs = 8000
#validation_every_n_secs= 10
validation_metric = accuracy
validation_metric_mode = max
save_after_valid = True
log_every_n_secs = 20



# parlai.core.dict:DictionaryAgent
;dict_language = chinese
;dict_class =  parlai.agents.bert_ranker.bert_dictionary:BertDictionaryAgent
;dict_tokenizer = bert
;dict_lower = True



;codes_attention_type = multihead
;codes_attention_num_heads = 12
;output_scaling = 0.06
;variant = xlm
;reduction_type = mean
;share_encoders = False
;learn_positional_embeddings = True
;n_layers = 12
;n_heads = 12
;ffn_size = 3072
;attention_dropout = 0.1
;relu_dropout = 0.0
;dropout = 0.1
;n_positions = 1024
;embedding_size = 768
;activation = gelu
;embeddings_scale = False
;n_segments = 2
;learn_embeddings = True