#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@file: 
@version: 
@desc:  
@time: 2021/3/29 16:21

@Modify Time      @Author    @Version    @Desciption
------------      -------    --------    -----------
2021/3/29 16:21   wangfc      1.0         None

"""
from datetime import datetime
from elasticsearch import Elasticsearch,helpers
import json

def build_elasticsearch_index(dataset,corpus,model,embedding_dim=312,chunk_size = 500):
    """
    datasetï¼š  æ•°æ®é›†åç§°
    embedding_dimï¼š embedding çš„ç»´åº¦

    """
    ids = list(corpus.keys())
    sentences = [corpus[id] for id in ids]

    es = Elasticsearch()
    # å»ºç«‹ç´¢å¼•ï¼šIndex data, if the index does not exists
    if not es.indices.exists(index=dataset):
        try:
            es_index = {
                "mappings": {
                    "properties": {
                        "question": {"type": "text"},
                        "question_vector": {"type": "dense_vector", "dims": embedding_dim}
                    }
                }
            }
            es.indices.create(index=dataset, body=es_index, ignore=[400])

            print("Index data (you can stop it by pressing Ctrl+C once):")
            with tqdm.tqdm(total=len(ids)) as pbar:
                for start_idx in range(0, len(ids), chunk_size):
                    end_idx = start_idx + chunk_size
                    # å°†å¥å­è¿›è¡Œç¼–ç 
                    embeddings = model.encode(sentences[start_idx:end_idx], show_progress_bar=False)
                    bulk_data = []
                    for qid, question, embedding in zip(ids[start_idx:end_idx], sentences[start_idx:end_idx],
                                                        embeddings):
                        bulk_data.append({
                            "_index": dataset,
                            "_id": qid,
                            "_source": {
                                "question": question,
                                "question_vector": embedding
                            }
                        })
                    helpers.bulk(es, bulk_data)
                    pbar.update(chunk_size)
        except:
            print("During index an exception occured. Continue\n\n")




def basic_operations():
    # é»˜è®¤hostä¸ºlocalhost,portä¸º9200.ä½†ä¹Ÿå¯ä»¥æŒ‡å®šhostä¸port
    es = Elasticsearch(hosts="10.20.32.187:9200")
    index = 'employer'
    # ä½¿ç”¨create()æ–¹æ³•åˆ›å»ºä¸€ä¸ªåä¸º *** çš„ Index
    result  = es.indices.create(index=index)
    # å¦‚æœåˆ›å»ºæˆåŠŸï¼Œåˆ™ä¼šè¿”å›ä¸‹é¢ç»“æœ
    print(result)
    # {'acknowledged': True, 'shards_acknowledged': True, 'index': 'names'}

    doc_type = "test-type"
    id_num = "1"
    # Index é‡Œé¢å•æ¡çš„è®°å½•ç§°ä¸º Documentï¼ˆæ–‡æ¡£ï¼‰ã€‚è®¸å¤šæ¡ Document æ„æˆäº†ä¸€ä¸ª Indexã€‚
    # Document ä½¿ç”¨ JSON æ ¼å¼è¡¨ç¤ºï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ã€‚
    data ={
      "user": "æå››",
      "title": "å·¥ç¨‹å¸ˆ",
      "desc": "ç³»ç»Ÿç®¡ç†"
    }
    # æ·»åŠ æˆ–æ›´æ–°æ•°æ®
    # index å‚æ•°ä»£è¡¨äº†ç´¢å¼•åç§°ï¼Œdoc_type ä»£è¡¨äº†æ–‡æ¡£ç±»å‹ï¼Œbody åˆ™ä»£è¡¨äº†æ–‡æ¡£å…·ä½“å†…å®¹ï¼Œid åˆ™æ˜¯æ•°æ®çš„å”¯ä¸€æ ‡è¯† IDã€‚
    # index () æ–¹æ³•åˆ™ä¸éœ€è¦ï¼Œå¦‚æœä¸æŒ‡å®š idï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª id
    result = es.index(index=index,
             doc_type=doc_type,
             id=id_num,
             body=data)

    # è·å–ç´¢å¼•ä¸ºmy_index,æ–‡æ¡£ç±»å‹ä¸ºtest_typeçš„æ‰€æœ‰æ•°æ®,resultä¸ºä¸€ä¸ªå­—å…¸ç±»å‹
    result = es.search(index=index,doc_type=doc_type)


    #æ›´æ–°æ•°æ®
    # æŒ‡å®šæ•°æ®çš„ id å’Œå†…å®¹ï¼Œè°ƒç”¨ update () æ–¹æ³•å³å¯ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ Elasticsearch å¯¹åº”çš„æ›´æ–° API å¯¹ä¼ é€’æ•°æ®çš„æ ¼å¼æ˜¯æœ‰è¦æ±‚çš„ï¼Œæ›´æ–°æ—¶ä½¿ç”¨çš„å…·ä½“ä»£ç å¦‚ä¸‹ï¼š
    data ={
        "doc":{
          "user": "æå››",
          "title": "å·¥ç¨‹å¸ˆ",
          "desc": "ç³»ç»Ÿç®¡ç†",
          "timestamp": datetime.now()
        }
    }
    result = es.update(index=index, doc_type=doc_type, body=data, id=1)
    print(result)
    # result å­—æ®µä¸º updatedï¼Œå³è¡¨ç¤ºæ›´æ–°æˆåŠŸï¼Œ _versionå­—æ®µï¼Œè¿™ä»£è¡¨æ›´æ–°åçš„ç‰ˆæœ¬å·æ•°ï¼Œ2 ä»£è¡¨è¿™æ˜¯ç¬¬äºŒä¸ªç‰ˆæœ¬ï¼Œå› ä¸ºä¹‹å‰å·²ç»æ’å…¥è¿‡ä¸€æ¬¡æ•°æ®ï¼Œæ‰€ä»¥ç¬¬ä¸€æ¬¡æ’å…¥çš„æ•°æ®æ˜¯ç‰ˆæœ¬ 1ã€‚


    # åˆ é™¤ Index
    result = es.indices.delete(index=index, ignore=[400, 404])
    print(result)


def create_index(hosts="10.20.32.187:9200",index='news',doc_type='politics'):
    # é»˜è®¤hostä¸ºlocalhost,portä¸º9200.ä½†ä¹Ÿå¯ä»¥æŒ‡å®šhostä¸port
    es = Elasticsearch(hosts= hosts)
    # ä½¿ç”¨create()æ–¹æ³•åˆ›å»ºä¸€ä¸ªåä¸º *** çš„ Index
    es.indices.create(index=index,ignore=[400])
    # å¦‚æœåˆ›å»ºæˆåŠŸï¼Œåˆ™ä¼šè¿”å›ä¸‹é¢ç»“æœ, {'acknowledged': True, 'shards_acknowledged': True, 'index': 'names'}
    # æ–°å»ºä¸€ä¸ªç´¢å¼•å¹¶æŒ‡å®šéœ€è¦åˆ†è¯çš„å­—æ®µ
    # mapping ä¿¡æ¯ä¸­æŒ‡å®šäº†åˆ†è¯çš„å­—æ®µï¼ŒğŸŒå…¶ä¸­å°† title å­—æ®µçš„ç±»å‹ type æŒ‡å®šä¸º textï¼Œ
    # å¹¶å°†åˆ†è¯å™¨ analyzer å’Œæœç´¢åˆ†è¯å™¨ search_analyzer è®¾ç½®ä¸º ik_max_word ï¼Œå³ä½¿ç”¨åˆšåˆšå®‰è£…çš„ä¸­æ–‡åˆ†è¯æ’ä»¶ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤ä½¿ç”¨è‹±æ–‡åˆ†è¯å™¨
    mapping = {
        'properties': {
            'title': {
                'type': 'text',
                'analyzer': 'ik_max_word',
                'search_analyzer': 'ik_max_word'
            }
        }
    }
    result = es.indices.put_mapping(index=index, doc_type=doc_type, body=mapping)
    print(result)

    return es


datas = [
    {
        'title': 'è®¾è®¡çµé­‚ç¦»èŒï¼Œèµ°ä¸‹ç¥å›çš„è‹¹æœè®¾è®¡å°†å»å‘ä½•æ–¹ï¼Ÿ',
        'url': 'https://www.tmtpost.com/4033397.html',
        'date': '2019-06-29 11:30'
    },
    {
        'title': 'åŒ»ç”Ÿä¸­çš„å»ºç­‘è®¾è®¡å¸ˆï¼Œå‡­ä»€ä¹ˆæŒ½æ•‘äº†ä¸Šä¸‡äººçš„ç”Ÿå‘½ï¼Ÿ',
        'url': 'https://www.tmtpost.com/4034052.html',
        'date': '2019-06-29 11:10'
    },
    {
        'title': 'ä¸­å›½ç½‘çº¢äºŒåå¹´ï¼šä»ç—å­è”¡ã€èŠ™è“‰å§å§åˆ°æä½³ç¦ï¼Œæµé‡ä¸å˜ç°çš„åšå¼ˆ',
        'url': 'https://www.tmtpost.com/4034045.html',
        'date': '2019-06-29 11:03'
    },
    {
        'title': 'ç½‘æ˜“äº‘éŸ³ä¹ã€å–œé©¬æ‹‰é›…ç­‰éŸ³é¢‘ç±»åº”ç”¨è¢«ä¸‹æ¶ï¼Œæˆ–å› è¿åç›¸å…³è§„å®š',
        'url': 'https://www.tmtpost.com/nictation/4034040.html',
        'date': '2019-06-29 10:07'
    }
]

def dsl_search(es, index='news', doc_type='politics',title='ç½‘çº¢ è®¾è®¡å¸ˆ' ):
    """
    Elasticsearch æ”¯æŒçš„ DSL è¯­å¥æ¥è¿›è¡ŒæŸ¥è¯¢ï¼Œä½¿ç”¨ match æŒ‡å®šå…¨æ–‡æ£€ç´¢ï¼Œæ£€ç´¢çš„å­—æ®µæ˜¯ title
    """
    dsl = {
        'query': {
            'match': {
                'title': title
            }
        }
    }
    result = es.search(index=index, doc_type=doc_type, body=dsl)
    print(json.dumps(result, indent=2, ensure_ascii=False))
    return result


def interactive_search(query=None,model=None,dataset='quora'):
    """
    #Interactive search queries
    äº¤äº’å¼æœç´¢
    """
    es = Elasticsearch()
    if query is None:
        query = input("Please enter a question: ")
    encode_start_time = time.time()
    question_embedding = model.encode(query)
    encode_end_time = time.time()

    # Lexical search
    bm25 = es.search(index=dataset, body={"query": {"match": {"question": query}}})

    # Sematic search
    sem_search = es.search(index=dataset, body={
          "query": {
            "script_score": {
              "query": {"match_all": {}},
              "script": {
                "source": "cosineSimilarity(params.queryVector, doc['question_vector']) + 1.0",
                "params": { "queryVector": question_embedding}
              }
            }
          }
        })

    print("Input question:", query)
    print("Computing the embedding took {:.3f} seconds, BM25 search took {:.3f} seconds, semantic search with ES took {:.3f} seconds".format(encode_end_time-encode_start_time, bm25['took']/1000, sem_search['took']/1000))

    print("BM25 results:")
    for hit in bm25['hits']['hits'][0:5]:
        print("\t{}".format(hit['_source']['question']))

    print("\nSemantic Search results:")
    for hit in sem_search['hits']['hits'][0:5]:
        print("\t{}".format(hit['_source']['question']))

    print("\n\n========\n")

if __name__ == '__main__':
    es = create_index()
    for data in datas:
        es.index(index='news', doc_type='politics', body=data)

    result = es.search(index='news', doc_type='politics')
    print(json.dumps(result, indent=4, ensure_ascii=False))


    result = dsl_search(es=es)




