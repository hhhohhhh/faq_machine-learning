中文分词根据实现原理和特点，主要分为以下2个类别：

常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。

# 1. 基于词典分词算法
也称字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已建立好的“充分大的”词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法分为以下几种：
   - 正向最大匹配法  
   - 逆向最大匹配法
   - 双向匹配分词法
   - 双向切词、最大切词、最小切词、最短路径匹配算法、最大最小切词  
   
   mmseg4J算法，论文《Word Identification_For_Mandarin Chinese Sentences》，1992年论文中总结的6条规律，歧义消除的成功率可以达到99.77%，至今仍然被广泛使用。  
   基于词典的分词算法是应用最广泛、分词速度最快的。很长一段时间内研究者都在对基于字符串匹配方法进行优化，比如最大长度设定、字符串存储和查找方式以及对于词表的组织结构，比如采用TRIE索引树、哈希索引等。

# 2. 传统统计学派 
这里的统计学派指的是狭义上的统计学派。基于统计学的分词方法一般也需要海量的中文语料库，统计挖掘语言的统计信息，结合构建语言模型来分词，如”N-Gram信息”。统计是非常有必要的，是数据挖掘的重要基石。该流派最出名的算法要数”最大概率分词算法”了，在一个大的语料库中，成词的字共同出现的概率是比较大的，这也是新词发现的理论基础。  
利用N-Gram和马尔可夫链原理，当前词的出现只与其前一个词出现有关，所以一个句子分词的概率就等于句中各个词语出现概率的乘积。利用这一思想，python语言编写的jieba中的最大概率分词方法，结合字典和动态规划，开发出史上最成功的中文分词算法，github上star超过20k。

# 3. 传统的机器学习算法
该流派最出名的算法要数”最大概率分词算法”了，在一个大的语料库中，成词的字共同出现的概率是比较大的，这也是新词发现的理论基础。  
这类目前常用的是算法是HMM、CRF、SVM、深度学习等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。  
   - 2002年Nianwen Xue在其论文《Combining Classifiers for Chinese Word Segmentation》  
首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，
   - 2003年《Chinese word segmentation as character tagging》  
   使用最大熵训练的字标注分词算法，在Bakeoff2003的测评中，OOV召回率取得了最好成绩，由此，字标注的方法开始得到重视，并迅速催生到ME、SVM、CRF和特征抽取的比拼上，其影响延续至今。  
   LTP、THULAC和pkuseg，使用的便是特征提取+CRF方案
   

# 4. 深度学习派 
该派别也是多使用有监督的学习方案，主张引入更多的外部信息（如字/词/句embedding、外部字典），以达到更好的分词效果。
在Bakeoff开放测试效果已经超出了传统机器学习方法。  
近来的热点，如多标准语料协同训练(如pku数据集和msra数据集)，多任务联合训练(如分词和词性标注)，前景更是不可估量。  
现在多用得比较多的网络架构还是Embedding + BI-LSTM+CRF等  
如万年老版本0.39的Jieba加入了paddlepaddle深度学习框架的双向GRU分词模型，  
HanLP也全部转python，上线了BERT等embedding的各种模型(tensorflow=2.1.0)，  
展露头角的Jiagu是基于tensorflow1.5和BiLSTM。  

# 5. 语义理解派
该派别认为要从句法分析、语义理解的基础上进行分词，如文法、依存句法等。不过似乎历史上并没有做出很大的成果，由于系统复杂，效果不佳，暂时没有了声音。近年来的联合训练，倒是可以勉强划分部分进入这个派别。


# 分词器当前存在问题：
目前中文分词难点主要有三个：
1. 分词标准：比如人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的。这需要根据不同的需求制定不同的分词标准。

2. 歧义：对同一个待切分字符串存在多个分词结果。歧义又分为组合型歧义、交集型歧义和真歧义三种类型。    
   1. 组合型歧义：
      - 粒度  
      分词是有不同的粒度的，指某个词条中的一部分也可以切分为一个独立的词条。
      比如“×××”，粗粒度的分词就是“×××”，细粒度的分词可能是“中华/人民/共和国”
      - 复合词的识别  
     (限定量词，重叠词，派生词，例如"一个个", "范范范玮琪", "上班族", "气功热")
      - 专有名词的识别(例如"哈工大SCIR实验室")
   
    2. 交集型歧义：  
   在“郑州天和服装厂”中，“天和”是厂名，是一个专有词，“和服”也是一个词，它们共用了“和”字。

    3. 真歧义：本身的语法和语义都没有问题, 即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果。例如：对于句子“美国会通过对台售武法案”，既可以切分成“美国/会/通过对台售武法案”，又可以切分成“美/国会/通过对台售武法案”。
3. 未登录问题  
(即新词，如网络新词"给力","坑爹", "盘它"等)；

一般在搜索引擎中，构建索引时和查询时会使用不同的分词算法。
常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度

# 部分分词器的简单说明
- 哈工大的分词器： 
主页上给过调用接口，每秒请求的次数有限制。

- 清华大学THULAC：
目前已经有Java、Python和C++版本，并且代码开源

- 斯坦福分词器：  
作为众多斯坦福自然语言处理中的一个包，目前最新版本3.7.0， Java实现的CRF算法。可以直接使用训练好的模型，也提供训练模型接口。

- Hanlp分词：  
求解的是最短路径。优点：开源、有人维护、可以解答。原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。

- 结巴分词工具：  
基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。

- ZPar分词器：  
新加坡科技设计大学开发的中文分词器，包括分词、词性标注和Parser，支持多语言，据说效果是公开的分词器中最好的，C++语言编写。

字嵌入+Bi-LSTM+CRF分词器：本质上是序列标注，这个分词器用人民日报的80万语料，据说按照字符正确率评估标准能达到97.5%的准确率，各位感兴趣可以去看看。
